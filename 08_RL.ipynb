{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarShao0124/deep_learning_practice/blob/main/08_RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gPhGQSZ1era"
      },
      "source": [
        "# Deep Reinforcement Learning: An Introduction\n",
        "\n",
        "In this tutorial, we will enter the world of Deep Reinforcement Learning (DRL). In particular, we will first familiarize ourselves with some basic concepts of Reinforcement Learning (RL), then we will implement a classical tabular Q-learning method for the classic [Frozen Lake](https://gym.openai.com/envs/FrozenLake-v0/) puzzle and finally, implement a Deep Q-learning approach for the [CartPole](https://gym.openai.com/envs/CartPole-v1/) problem.\n",
        "\n",
        "\n",
        ">\n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://media2.giphy.com/media/46ib09ZL1SdWuREnj3/giphy.gif?cid=3640f6095c6e92762f3446634d90bc65) ![alt text](https://media0.giphy.com/media/d9QiBcfzg64Io/200w.webp?cid=3640f6095c6e93e92f30655873731752)![alt text](https://i.gifer.com/GpAY.gif)\n",
        "\n",
        "The gifs above, show the results obtained by [Deepmind](https://arxiv.org/pdf/1312.5602v1.pdf) in one of their latest papers. They successfully trained an RL agent using deep Q-learning to play classical Atari arcade games. Let's see now how they did it.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmpPzxlbwvxI"
      },
      "source": [
        "# Q-Learning\n",
        "\n",
        "This family of RL methods try to learn an approximator of the action-value functions $Q(s,a)$  based on the [Bellman equation](https://en.wikipedia.org/wiki/Bellman_equation), such that the update using a classical [gradient descent ](https://en.wikipedia.org/wiki/Gradient_descent) formulation is given by:\n",
        "$$Q\\left(s,a\\right)=Q\\left(s,a\\right)+ \\alpha \\left(r+\\gamma \\max _{a} Q\\left(s_{t+1},a\\right)-Q\\left(s,a\\right)\\right).$$\n",
        "Where $\\alpha$ is the step size.\n",
        " Q-Learning updates the estimated reward at each time step and  uses the old estimate $ \\max _{a}Q\\left(s_{t+1},a\\right)$ to update the new ones. In a more algorithmic way, the Q-Learning process is the following:\n",
        "\n",
        "\n",
        "1.   Initialize Q-values at random $Q\\left(s,a\\right)$.\n",
        "2. Forever or until learning is stopped do:\n",
        "> 1.  Observe state $s$.\n",
        "> 2.   Take action $a$ according to your policy, e.g., $\\epsilon$-greedy.\n",
        "> 3.   Observe reward $r$ and new state $s_{t+1}$.\n",
        "> 4. Based on your actual estimates, compute $\\max _{a}Q\\left(s_{t+1},a\\right)$.\n",
        "> 5. Update your current estimate for  $Q\\left(s,a\\right)$:\n",
        "$$Q\\left(s,a\\right)=Q\\left(s,a\\right)+ \\alpha \\left(r+\\gamma \\max _{a} Q\\left(s_{t+1},a\\right)-Q\\left(s,a\\right)\\right).$$\n",
        "\n",
        "Okay, now that we are familiar with Q-Learning lets jump to a real implementation of it.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_9McSwZh5sT"
      },
      "source": [
        "\n",
        "## Tabular Q-Learning with Frozen Lake\n",
        "In this section we will teach an agent how to play  the [Frozen lake](https://gym.openai.com/envs/FrozenLake-v0/) game using a classical tabular Q-learning. Brace yourselves, winter is coming!\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/1ee37cfc3130057f828f19b3cee6066d41c1eeb4/Q%20learning/FrozenLake/frozenlake.png)\n",
        "\n",
        "Winter has arrived and you and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so you must navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.\n",
        "The goal of this game is to go from the starting state (S) to the goal state (G) by walking only on frozen tiles (F) and avoid holes (H). However, the ice is slippery (!!), so you won't always move in the direction you intend (stochastic environment), i.e., there is a probability $p$ that you move in the direction selected and a probability $(1-p)$ that given the slippery ice, you move to a random position near position. Specifically, let's say you select the action UP, you have a probability of 1/3 of actually going UP, 1/3 of going RIGHT and 1/3 of going LEFT. Similarly, if you select LEFT, you have a probability of 1/3 of actually going LEFT, 1/3 of going UP and 1/3 of going DOWN.\n",
        "\n",
        "The lake is represented by a 4x4 grid and the location where the frisbee has landed (G) as well as the holes (H) is always the same for every new game. The game is restarted every time you have successfully recovered the frisbee or you have fallen into the cold waters. A reward of +1 is given every time you recover the frisbee and 0 other way.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNrutC72jZRZ"
      },
      "source": [
        "**Step 0: Import the needed libraries:**\n",
        "\n",
        "We will be using 3 libraries:\n",
        "\n",
        "* Numpy for our Qtable.\n",
        "* OpenAI Gym for our FrozenLake Environment\n",
        "* Random to generate random numbers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0OxrnpgjyFh"
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium --upgrade\n",
        "\n",
        "import base64\n",
        "import collections\n",
        "import glob\n",
        "import io\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "from IPython.display import HTML\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers, models, optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-Nx8PYnj4I5"
      },
      "source": [
        "**Environment creation:**\n",
        "\n",
        "OpenAi is  a library composed of many environments that we can use to train our agents, in our case we choose to use the Frozen Lake."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWgurerLkNLe"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"FrozenLake-v1\", render_mode='rgb_array')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeaPjILgkS7Z"
      },
      "source": [
        "**Q-table**\n",
        "\n",
        " Now, we'll create our Q-table. The goal of the Q-table is to store the estimates $Q\\left(s,a\\right)$ and retrieve them when necessary. In this game the states are represented by each of the 16 grid positions being 0 the starting position and 16 the goal position and the actions are 4: left, right, up and down. Our Q-table will have then $16 \\times 4$ positions, where the value of the first column of the first row represents the expected return of being in position 0 and taking left.\n",
        "\n",
        "The number of rows (states) and columns (actions) the table will have can also be obtained using the values action_size and the state_size from the OpenAI Gym library: *env.action_space.n* and* env.observation_space.n*.\n",
        "\n",
        "We initialize the table to 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eintO6cYk5qN"
      },
      "outputs": [],
      "source": [
        "action_size = env.action_space.n\n",
        "state_size = env.observation_space.n\n",
        "qtable = np.zeros((state_size, action_size))\n",
        "print(qtable)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G06GlU14k_JG"
      },
      "source": [
        "**Hyperparameters**\n",
        "\n",
        "Following, we specify the hyperparameters:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYWdb_rHlFzq"
      },
      "outputs": [],
      "source": [
        "total_episodes = 25000        # Total episodes\n",
        "learning_rate = 0.8           # Learning rate (alpha in the previous formulation)\n",
        "max_steps = 99                # Max steps per episode\n",
        "gamma = 0.95                  # Discounting rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVb-8n8Jlkhs"
      },
      "source": [
        "At first, we don't know how to interact with the environment (Q-table values set to 0), so we start exploring it by taking a random action with probability $\\epsilon=1$, capturing the rewards obtained and updating the Q-values of the table accordingly. As time passes by, we start knowing more and more the environment, so we reduce (decay_rate) the probability of taking a random action and we start exploiting our knowledge, we choose the action that leads us to the highest reward, i.e., the one with the highest Q-value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXq90uCMllKE"
      },
      "outputs": [],
      "source": [
        "# Exploration parameters\n",
        "epsilon = 1.0                 # Exploration rate\n",
        "max_epsilon = 1.0             # Exploration probability at start\n",
        "min_epsilon = 0.01            # Minimum exploration probability\n",
        "decay_rate = 0.005             # Exponential decay rate for exploration prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVu4B3F2lLl8"
      },
      "source": [
        "**Q-Learning**\n",
        "\n",
        "Now we implement the Q-Learning algorithm:\n",
        "> 1.  Observe state $s$.\n",
        "> 2.   Choose a random value $v$ between 0 and 1.\n",
        "> 3. If $v<\\epsilon$, we choose a random action, otherwise we select the action with maximum $Q(s,a)$.\n",
        "> 3.   Observe reward $r$ and new state $s_{t+1}$.\n",
        "> 4. Based on your previous estimates, compute $\\max _{a}Q\\left(s_{t+1},a\\right)$.\n",
        "> 5. Update your current estimates for  $Q\\left(s,a\\right)$:\n",
        "$$Q\\left(s,a\\right)=Q\\left(s,a\\right)+ \\alpha \\left(r+\\gamma \\max _{a} Q\\left(s_{t+1},a\\right)-Q\\left(s,a\\right)\\right).$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYbqMpg-liRd"
      },
      "outputs": [],
      "source": [
        "# List of rewards\n",
        "rewards = []\n",
        "\n",
        "for episode in range(total_episodes):\n",
        "    # Reset the environment\n",
        "    state, _ = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards = 0\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        # 3. Choose an action a in the current world state (s)\n",
        "        ## First we randomize a number\n",
        "        exp_exp_tradeoff = random.uniform(0, 1)\n",
        "\n",
        "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
        "        if exp_exp_tradeoff > epsilon:\n",
        "            action = np.argmax(qtable[state,:])\n",
        "\n",
        "        # Else doing a random choice --> exploration\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "        new_state, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "        # qtable[new_state,:] : all the actions we can take from new state\n",
        "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
        "\n",
        "        total_rewards += reward\n",
        "\n",
        "        # Our new state is state\n",
        "        state = new_state\n",
        "\n",
        "        # If done (if we're dead) : finish episode\n",
        "        if done == True:\n",
        "            break\n",
        "\n",
        "    # Reduce epsilon (because we need less and less exploration)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
        "    rewards.append(total_rewards)\n",
        "\n",
        "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
        "print(qtable)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvcHhVx5lrWZ"
      },
      "source": [
        "**Use our Q-table to play FrozenLake!**\n",
        "\n",
        "After 25000 episodes, our Q-table can be used as a \"cheatsheet\" to play FrozenLake\"!\n",
        "  \n",
        "By running this cell, you can see our agent playing FrozenLake:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hz65tCGSlzSc"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"FrozenLake-v1\", render_mode='rgb_array')\n",
        "state, _ = env.reset()\n",
        "step = 0\n",
        "\n",
        "plt.imshow(env.render())\n",
        "plt.show()\n",
        "\n",
        "for step in range(max_steps):\n",
        "\n",
        "    # Take the action (index) that have the maximum expected future reward given that state\n",
        "    action = np.argmax(qtable[state,:])\n",
        "\n",
        "    new_state, reward, terminated, truncated, info = env.step(action)\n",
        "    plt.imshow(env.render())\n",
        "    plt.show()\n",
        "\n",
        "    # We print the current step.\n",
        "    print(f\"Number of steps: {step}\")\n",
        "    if terminated or truncated:\n",
        "      break\n",
        "    state = new_state\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSroWmlroYiC"
      },
      "source": [
        "Let‚Äôs see how many times our agent finds the frisbee üéâ\n",
        "\n",
        "To this end we will print the last step of the game."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3BUEqOzocSl"
      },
      "outputs": [],
      "source": [
        "games=5\n",
        "for game in range(games):\n",
        "    env = gym.make(\"FrozenLake-v1\")\n",
        "    state, _ = env.reset()\n",
        "    step = 0\n",
        "    for step in range(max_steps):\n",
        "\n",
        "        # Take the action (index) that have the maximum expected future reward given that state\n",
        "        action = np.argmax(qtable[state,:])\n",
        "        new_state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "        if terminated or truncated:\n",
        "        # Here, we decide to only print the last state (to see if our agent is on the goal or fall into a hole)\n",
        "        # We print the number of step it took.\n",
        "            print(f\"Number of steps: {step}\")\n",
        "            break\n",
        "        state = new_state\n",
        "    env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfLNsIPtQevL"
      },
      "outputs": [],
      "source": [
        "games = 5\n",
        "total_rewards = 0\n",
        "\n",
        "for game in range(games):\n",
        "    env = gym.make(\"FrozenLake-v1\")\n",
        "    state, _ = env.reset()\n",
        "    step = 0\n",
        "    for step in range(max_steps):\n",
        "        # Take the action (index) that have the maximum expected future reward given that state\n",
        "        action = np.argmax(qtable[state,:])\n",
        "        new_state, reward, terminated, truncated, info = env.step(action)\n",
        "        if terminated or truncated:\n",
        "            total_rewards += reward\n",
        "            break\n",
        "        state = new_state\n",
        "    env.close()\n",
        "success = total_rewards / games\n",
        "print(\"Ratio of sucessfully finished episodes is {:f}\".format(success))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_d3L77JNpUvy"
      },
      "source": [
        "## CartPole\n",
        "\n",
        "That wasn't so hard! How about trying to balance a pole so it does not fall? In this section we will address the [CartPole](https://gym.openai.com/envs/CartPole-v1/) problem, let's get to it!\n",
        "\n",
        "![texto alternativo](https://keon.github.io/images/deep-q-learning/animation.gif)\n",
        "\n",
        "As before we will use Q-learning to train our agent, so let's start by constructing our Q-table. We first need to find out the number of columns and rows of it. By checking the environment specifications of [OpenAi](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py), we see that the actions are left and right, so we need two columns for the actions. On the other hand, the state information is given by:\n",
        "\n",
        "        Num\tObservation                 Min         Max\n",
        "        0\tCart Position             -4.8            4.8\n",
        "        1\tCart Velocity             -Inf            Inf\n",
        "        2\tPole Angle                 -24 deg        24 deg\n",
        "        3\tPole Velocity At Tip      -Inf            Inf\n",
        "      \n",
        "The cart position goes from -4.8 to 4.8 with a resolution of 0.01, which means $\\frac{4.8 \\times 2}{0.01}=960$ possible carts positions, while the cart velocity goes from $-\\infty$ to $\\infty$!. How we are going to construct a table with $\\infty$ rows?\n",
        "\n",
        "Do not panic! That is when deep learning steps up and takes over the stage. As you have already seen the use of Deep Neural Networks as general function approximators have been proven to work very well in a wide range of areas, reinforcement learning is not an exception. In this case we will use the NNs as function approximation between the mapping of states to actions, so for every input state, we want the NNs to output an approximation of the $Q\\left(s,a\\right)$.\n",
        "\n",
        "![alt text](https://proxy.duckduckgo.com/iu/?u=https%3A%2F%2Fcdn-images-1.medium.com%2Fmax%2F1318%2F1*Gh5PS4R_A5drl5ebd_gNrg%402x.png&f=1)\n",
        "\n",
        "In this particular scenario, the input layer will have the same number of inputs as environment parameters, 4, and the output layer will have the same number of outputs as actions, in this case 2.\n",
        "\n",
        "**Reward:** A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHi3LRul2b6h"
      },
      "source": [
        "\n",
        "**Step 0: Import the needed libraries**\n",
        "\n",
        "We start by importing the needed libraries:\n",
        "We will be using 3 libraries:\n",
        "* Keras: for our DNNs.\n",
        "* OpenAI Gym: for our CartPole Environment\n",
        "* Random: to generate random numbers.\n",
        "* Collections: Collection will be use to create a memory buffer to store the tuples $\\left(S_t, A_t, R_t,S_{t+1}\\right)$ of transactions.\n",
        "\n",
        "The idea behind the use of a memory buffer is that most optimization algorithms, including gradient descent, assume that the samples used in an update step are independent and identically distributed. Clearly in the defined environment that is not the case, however, by sampling uniformly the memory buffer with a high number of samples the correlation between contiguous samples is broken and less likely to be correlated samples are used to update the network's weights, leading to a stable optimization of the action-parameter selection.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KDyanK23SQH"
      },
      "source": [
        "**The Agent**\n",
        "\n",
        "Let's start by coding a general DQ-Learning agent. The state and action size are passed as parameters and we configure a replay buffer to have capacity to store 2000 experienced transitions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18b_7T9K-V_a"
      },
      "outputs": [],
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = collections.deque(maxlen=2000)\n",
        "        self.gamma = 0.95    # discount rate\n",
        "        self.epsilon = 1.0  # exploration rate\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.learning_rate = 0.001\n",
        "        self.model = self._build_model()\n",
        "\n",
        "    # Now we address the DNNs; we are going to use two fully connected layers of 24 neurons each and as an optimizer we select Adam.\n",
        "    def _build_model(self):\n",
        "        # Neural Net for Deep-Q learning Model\n",
        "        model = models.Sequential([\n",
        "            layers.Input(shape=(self.state_size,)),\n",
        "            layers.Dense(24, activation='relu'),\n",
        "            layers.Dense(24, activation='relu'),\n",
        "            layers.Dense(self.action_size, activation='linear')\n",
        "        ])\n",
        "        model.compile(loss='mse', optimizer=optimizers.Adam(learning_rate=self.learning_rate))\n",
        "        return model\n",
        "\n",
        "    # Now define the method to store the transitions into the memory buffer.\n",
        "    # The parameter done is a boolean returned true when the pole has fallen.\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    # Now we implement an ùúñ-greedy policy.\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        act_values = self.model.predict(state)\n",
        "        return np.argmax(act_values[0]) # returns action\n",
        "\n",
        "    def exploit(self, state): # When we test the agent we dont want it to explore anymore, but to exploit what it has learnt\n",
        "        act_values = self.model.predict(state)\n",
        "        return np.argmax(act_values[0])\n",
        "\n",
        "    \"\"\"\n",
        "    Then comes the implementation of the Q-Learning method:\n",
        "    1. We obtain the samples to train the DNN from the replay buffer.\n",
        "    2. We compute $target=r+\\gamma \\max _{a} Q\\left(s_{t+1},a\\right)$, by doing a forward pass using next_state value.\n",
        "    3. We do a forward pass through the network to obtain the $Q\\left(s,a\\right)$ for all the possible actions.\n",
        "    4. In order to just update the parameter of the action taken, we copy target to the value of the $Q\\left(s,a\\right)$ of the actual $a$ taken.\n",
        "    5. We update the parameters of the network using MSE as loss function.\n",
        "    \"\"\"\n",
        "    def replay(self, batch_size):\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        ### This code below generates batches of states, actions, rewards\n",
        "        ### next states out of the sampled minibatch\n",
        "        state_b = np.squeeze(np.array(list(map(lambda x: x[0], minibatch))))\n",
        "        action_b = np.squeeze(np.array(list(map(lambda x: x[1], minibatch))))\n",
        "        reward_b = np.squeeze(np.array(list(map(lambda x: x[2], minibatch))))\n",
        "        next_state_b = np.squeeze(np.array(list(map(lambda x: x[3], minibatch))))\n",
        "        done_b = np.squeeze(np.array(list(map(lambda x: x[4], minibatch))))\n",
        "\n",
        "        target = (reward_b + self.gamma *\n",
        "                        np.amax(self.model.predict(next_state_b), 1))\n",
        "        target[done_b==1] = reward_b[done_b==1]\n",
        "        target_f = self.model.predict(state_b)\n",
        "        for k in range(target_f.shape[0]):\n",
        "            target_f[k][action_b[k]] = target[k]\n",
        "        self.model.train_on_batch(state_b, target_f)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    # Load, save models\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6YDZhTIDjiC"
      },
      "source": [
        "**Main**\n",
        "\n",
        "Following we implement the training of the agent. (Warning: it takes a while...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwA2WXrnDpzV"
      },
      "outputs": [],
      "source": [
        "EPISODES = 200\n",
        "env = gym.make('CartPole-v1')\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "agent = DQNAgent(state_size, action_size)\n",
        "batch_size = 32\n",
        "\n",
        "for e in range(EPISODES):\n",
        "    state, _ = env.reset()\n",
        "    state = np.reshape(state, [1, state_size])\n",
        "    for time in range(200):\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        next_state = np.reshape(next_state, [1, state_size])\n",
        "        agent.remember(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        if done:\n",
        "            print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
        "                  .format(e, EPISODES, time, agent.epsilon))\n",
        "            break\n",
        "        if len(agent.memory) > batch_size:\n",
        "            agent.replay(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2TqM_HhGVM6"
      },
      "source": [
        "Let's now visualize how the agent is performing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WO20MXmfNs4E"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else:\n",
        "    print(\"Could not find video\")\n",
        "\n",
        "\n",
        "def wrap_env(env):\n",
        "  env = gym.wrappers.RecordVideo(env, './video')\n",
        "  return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKv6DNBDGY0N"
      },
      "outputs": [],
      "source": [
        "env = wrap_env(gym.make('CartPole-v1', render_mode='rgb_array'))\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "state, _ = env.reset()\n",
        "state = np.reshape(state, [1, state_size])\n",
        "for time in range(200):\n",
        "    screen = env.render()\n",
        "    action = agent.exploit(state)\n",
        "    state, reward, terminated, truncated, _ = env.step(action)\n",
        "    if terminated or truncated:\n",
        "      break\n",
        "    state = np.reshape(state, [1, state_size])\n",
        "\n",
        "env.close()\n",
        "show_video()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wg2wp6Duu7Ok"
      },
      "source": [
        "You can have a look of the tutorials and code prepared by [OpenAI](https://spinningup.openai.com/en/latest/user/introduction.html) for further details on RL."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWgV2gkTfpCc"
      },
      "source": [
        "# Coursework\n",
        "\n",
        "## Task 1: On-policy vs. Off-policy\n",
        "Use the code given below to run the training loop, where the agent is trained for 200 episodes. The agent we give follows a Q-learning approach, which is an off-policy approach. You will now change the approach to SARSA, which is an on-policy approach. Also, for both Q-learning and SARSA test two different policies: $\\epsilon$-greedy and Softmax. $\\epsilon$-greedy is already defined in the tutorial and implemented in the given agent. Softmax policy refers to sampling the next action following the probability distribution given by $Softmax(Q(s, a))$. We provide you the NumPy softmax function to normalize the Q-Values into a probability function to use before sampling. Similarly to RNN, in the softmax function, there is a temperature value involved, we set a default value that works, but you can tweak it if you find another value with better performance. Report the new value if you decide to do so.\n",
        "\n",
        "You will need to modify `act` and `replay` from the `DQNAgent` to implement the different approaches we ask for. Results may differ from run to run due to different initialization states.\n",
        "\n",
        "**Report**\n",
        "* Plot the average reward for the last 50 episodes vs. number of training episodes (train for 200 episodes) for the four agents trained: Q-learning and SARSA with both $\\epsilon$-greedy policy and Softmax policy. Attach in the Appendix the modifications done to `DQNAgent` to implement the different agents. Do not include your code, a simple explanation with the key modifications is enough."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A47sXwdmhlrU"
      },
      "outputs": [],
      "source": [
        "def softmax(x, temperature=0.025):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    x = (x - np.expand_dims(np.max(x, 1), 1))\n",
        "    x = x/temperature\n",
        "    e_x = np.exp(x)\n",
        "    return e_x / (np.expand_dims(e_x.sum(1), -1) + 1e-5)\n",
        "\n",
        "class DQNAgent:\n",
        "  def __init__(self, state_size, action_size):\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    self.memory = collections.deque(maxlen=20000)\n",
        "    self.gamma = 0.95    # discount rate\n",
        "    self.epsilon = 1.0  # exploration rate\n",
        "    self.epsilon_min = 0.01\n",
        "    self.epsilon_decay = 0.995\n",
        "    self.learning_rate = 0.001\n",
        "    self.model = self._build_model()\n",
        "\n",
        "\n",
        "  def _build_model(self):\n",
        "    # Neural Net for Deep-Q learning Model\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Input(shape=(self.state_size,)))\n",
        "    model.add(layers.Dense(24, activation='relu'))\n",
        "    model.add(layers.Dense(48, activation='relu'))\n",
        "    model.add(layers.Dense(self.action_size, activation='linear'))\n",
        "    model.compile(loss='mse',\n",
        "                  optimizer=optimizers.Adam(learning_rate=self.learning_rate))\n",
        "    return model\n",
        "\n",
        "  def remember(self, state, action, reward, next_state, done):\n",
        "    self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "  def act(self, state, policy):# We implement the epsilon-greedy policy\n",
        "    if policy == \"epsilon-greedy\":\n",
        "      if np.random.rand() <= self.epsilon:\n",
        "          return random.randrange(self.action_size)\n",
        "      act_values = self.model.predict(state,verbose=0)\n",
        "      return np.argmax(act_values[0])\n",
        "\n",
        "    elif policy == \"softmax\":\n",
        "      act_values = self.model.predict(state, verbose=0)\n",
        "      act_values_2d = act_values.reshape(1, -1)  # shape (1, action_size)\n",
        "      probabilities = softmax(act_values_2d)[0]  # now you can do axis=1\n",
        "      return np.random.choice(self.action_size, p=probabilities)\n",
        "\n",
        "    else:\n",
        "      raise ValueError(\"Invalid policy. Choose either 'epsilon-greedy' or 'softmax'.\")\n",
        "\n",
        "  def exploit(self, state): # When we test the agent we dont want it to explore anymore, but to exploit what it has learnt\n",
        "    act_values = self.model.predict(state,verbose=0)\n",
        "    return np.argmax(act_values[0])\n",
        "\n",
        "  def replay(self, batch_size, mode, policy):\n",
        "    minibatch = random.sample(self.memory, batch_size)\n",
        "\n",
        "    state_b = np.squeeze(np.array(list(map(lambda x: x[0], minibatch))))\n",
        "    action_b = np.squeeze(np.array(list(map(lambda x: x[1], minibatch))))\n",
        "    reward_b = np.squeeze(np.array(list(map(lambda x: x[2], minibatch))))\n",
        "    next_state_b = np.squeeze(np.array(list(map(lambda x: x[3], minibatch))))\n",
        "    done_b = np.squeeze(np.array(list(map(lambda x: x[4], minibatch))))\n",
        "\n",
        "    ### Q-learning\n",
        "    if mode == \"Q-learning\":\n",
        "      target = (reward_b + self.gamma *\n",
        "                        np.amax(self.model.predict(next_state_b,verbose=0), 1))\n",
        "\n",
        "    ### SARSA\n",
        "    elif mode == \"SARSA\":\n",
        "      next_actions = np.array([self.act(np.reshape(next_state, [1, self.state_size]), policy)\n",
        "                             for next_state in next_state_b])  # Select next action with policy\n",
        "      next_q_values = self.model.predict(next_state_b, verbose=0)\n",
        "      target = reward_b + self.gamma * next_q_values[np.arange(len(next_state_b)), next_actions]\n",
        "\n",
        "    # other mode input\n",
        "    else:\n",
        "      raise ValueError(\"Invalid mode. Choose either 'Q-learning' or 'SARSA'.\")\n",
        "\n",
        "\n",
        "    target[done_b == 1] = reward_b[done_b == 1] # Update targets for terminal states\n",
        "    target_f = self.model.predict(state_b, verbose=0) # Predict Q-values for current states\n",
        "    for k in range(target_f.shape[0]):\n",
        "      target_f[k][action_b[k]] = target[k]\n",
        "    self.model.train_on_batch(state_b, target_f)\n",
        "    if self.epsilon > self.epsilon_min:\n",
        "      self.epsilon *= self.epsilon_decay\n",
        "\n",
        "  def load(self, name):\n",
        "    self.model.load_weights(name)\n",
        "  def save(self, name):\n",
        "    self.model.save_weights(name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q_epsilon_greedy_avg_reward = []\n",
        "Q_softmax_avg_reward = []\n",
        "SARSA_epsilon_greedy_avg_reward = []\n",
        "SARSA_softmax_avg_reward =[]"
      ],
      "metadata": {
        "id": "91kqw59RVF3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiW8Mr6VfvgM"
      },
      "outputs": [],
      "source": [
        "Q_softmax_avg_reward = []\n",
        "\n",
        "EPISODES = 200\n",
        "env = gym.make('CartPole-v1')\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "policy = \"softmax\"\n",
        "mode = \"SARSA\"\n",
        "agent = DQNAgent(state_size, action_size)\n",
        "batch_size = 32\n",
        "episode_reward_list = collections.deque(maxlen=50)\n",
        "\n",
        "\n",
        "for e in range(EPISODES):\n",
        "    state, _ = env.reset()\n",
        "    state = np.reshape(state, [1, state_size])\n",
        "    total_reward = 0\n",
        "    for time in range(200):\n",
        "      action = agent.act(state,policy)\n",
        "      next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "      done = terminated or truncated\n",
        "      total_reward += reward\n",
        "      next_state = np.reshape(next_state, [1, state_size])\n",
        "      agent.remember(state, action, reward, next_state, done)\n",
        "      state = next_state\n",
        "      if done:\n",
        "          break\n",
        "      if len(agent.memory) > batch_size:\n",
        "          agent.replay(batch_size,mode,policy)\n",
        "    episode_reward_list.append(total_reward)\n",
        "    episode_reward_avg = np.array(episode_reward_list).mean()\n",
        "    SARSA_softmax_avg_reward.append(episode_reward_avg)\n",
        "    print(\"episode: {}/{}, score: {}, e: {:.2}, last 50 ep. avg. rew.: {:.2f}\"\n",
        "                .format(e, EPISODES, total_reward, agent.epsilon, episode_reward_avg))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(SARSA_softmax_avg_reward)\n"
      ],
      "metadata": {
        "id": "mi4f3JSpdk9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SARSA_epsilon_greedy_avg_reward = [np.float64(11.0), np.float64(19.0), np.float64(16.333333333333332), np.float64(16.5), np.float64(18.6), np.float64(18.833333333333332), np.float64(21.0), np.float64(20.5), np.float64(19.555555555555557), np.float64(19.3), np.float64(19.272727272727273), np.float64(18.583333333333332), np.float64(17.923076923076923), np.float64(17.357142857142858), np.float64(16.8), np.float64(16.4375), np.float64(16.058823529411764), np.float64(15.777777777777779), np.float64(15.473684210526315), np.float64(15.2), np.float64(14.952380952380953), np.float64(14.636363636363637), np.float64(14.73913043478261), np.float64(14.5), np.float64(14.56), np.float64(14.346153846153847), np.float64(14.148148148148149), np.float64(14.035714285714286), np.float64(13.89655172413793), np.float64(13.866666666666667), np.float64(14.193548387096774), np.float64(14.5), np.float64(14.787878787878787), np.float64(15.294117647058824), np.float64(15.428571428571429), np.float64(15.444444444444445), np.float64(15.621621621621621), np.float64(15.789473684210526), np.float64(16.05128205128205), np.float64(16.525), np.float64(16.975609756097562), np.float64(18.166666666666668), np.float64(18.46511627906977), np.float64(18.636363636363637), np.float64(19.4), np.float64(20.434782608695652), np.float64(20.659574468085108), np.float64(20.895833333333332), np.float64(21.591836734693878), np.float64(22.04), np.float64(23.1), np.float64(23.72), np.float64(24.52), np.float64(25.24), np.float64(25.86), np.float64(27.56), np.float64(29.94), np.float64(31.2), np.float64(33.36), np.float64(34.94), np.float64(37.7), np.float64(41.34), np.float64(42.54), np.float64(46.34), np.float64(48.34), np.float64(50.7), np.float64(52.92), np.float64(55.96), np.float64(58.54), np.float64(62.34), np.float64(64.58), np.float64(66.98), np.float64(70.64), np.float64(73.34), np.float64(76.6), np.float64(80.42), np.float64(84.24), np.float64(87.46), np.float64(91.06), np.float64(94.36), np.float64(97.74), np.float64(100.78), np.float64(104.3), np.float64(107.66), np.float64(111.26), np.float64(114.94), np.float64(117.92), np.float64(121.48), np.float64(124.88), np.float64(128.18), np.float64(131.42), np.float64(133.18), np.float64(136.56), np.float64(139.58), np.float64(142.14), np.float64(144.22), np.float64(147.6), np.float64(150.22), np.float64(152.78), np.float64(155.16), np.float64(157.26), np.float64(159.42), np.float64(162.16), np.float64(164.78), np.float64(166.8), np.float64(167.36), np.float64(167.66), np.float64(169.58), np.float64(170.16), np.float64(171.72), np.float64(172.58), np.float64(172.6), np.float64(174.08), np.float64(172.92), np.float64(173.52), np.float64(173.96), np.float64(174.42), np.float64(174.28), np.float64(174.86), np.float64(173.34), np.float64(174.02), np.float64(175.46), np.float64(174.44), np.float64(174.34), np.float64(173.7), np.float64(172.16), np.float64(170.98), np.float64(170.08), np.float64(169.34), np.float64(168.62), np.float64(167.4), np.float64(166.86), np.float64(166.18), np.float64(165.02), np.float64(164.08), np.float64(162.56), np.float64(161.74), np.float64(160.4), np.float64(159.74), np.float64(158.42), np.float64(157.42), np.float64(156.84), np.float64(156.06), np.float64(156.52), np.float64(155.22), np.float64(154.26), np.float64(153.04), np.float64(152.1), np.float64(151.82), np.float64(151.34), np.float64(151.14), np.float64(150.26), np.float64(148.92), np.float64(147.72), np.float64(146.86), np.float64(146.54), np.float64(146.02), np.float64(146.5), np.float64(145.72), np.float64(145.26), np.float64(144.62), np.float64(143.44), np.float64(144.3), np.float64(143.68), np.float64(144.22), np.float64(143.62), np.float64(143.92), np.float64(143.76), np.float64(142.78), np.float64(143.26), np.float64(143.28), np.float64(141.78), np.float64(141.34), np.float64(140.9), np.float64(140.9), np.float64(141.42), np.float64(141.76), np.float64(141.8), np.float64(140.88), np.float64(141.6), np.float64(141.96), np.float64(141.84), np.float64(141.14), np.float64(141.46), np.float64(141.06), np.float64(140.84), np.float64(140.46), np.float64(140.64), np.float64(140.08), np.float64(140.06), np.float64(140.16), np.float64(141.08), np.float64(140.78), np.float64(139.28), np.float64(139.86), np.float64(140.9), np.float64(140.64), np.float64(141.12), np.float64(139.98), np.float64(140.9)]\n",
        "SARSA_softmax_avg_reward =\n",
        "Q_epsilon_greedy_avg_reward = [np.float64(59.0), np.float64(45.5), np.float64(34.0), np.float64(31.75), np.float64(27.2), np.float64(26.333333333333332), np.float64(24.571428571428573), np.float64(23.375), np.float64(22.666666666666668), np.float64(21.5), np.float64(20.90909090909091), np.float64(19.916666666666668), np.float64(19.153846153846153), np.float64(18.5), np.float64(17.933333333333334), np.float64(17.375), np.float64(17.11764705882353), np.float64(17.444444444444443), np.float64(17.0), np.float64(16.65), np.float64(16.285714285714285), np.float64(16.045454545454547), np.float64(15.782608695652174), np.float64(15.5), np.float64(15.32), np.float64(15.153846153846153), np.float64(15.0), np.float64(15.035714285714286), np.float64(14.931034482758621), np.float64(14.766666666666667), np.float64(14.580645161290322), np.float64(14.5), np.float64(14.393939393939394), np.float64(14.235294117647058), np.float64(14.228571428571428), np.float64(14.083333333333334), np.float64(13.972972972972974), np.float64(14.131578947368421), np.float64(14.076923076923077), np.float64(13.925), np.float64(13.78048780487805), np.float64(13.80952380952381), np.float64(17.0), np.float64(17.727272727272727), np.float64(18.244444444444444), np.float64(19.5), np.float64(20.46808510638298), np.float64(21.604166666666668), np.float64(22.632653061224488), np.float64(23.94), np.float64(25.1), np.float64(26.26), np.float64(27.62), np.float64(28.48), np.float64(29.28), np.float64(32.36), np.float64(33.48), np.float64(34.2), np.float64(35.72), np.float64(36.64), np.float64(38.4), np.float64(39.84), np.float64(40.84), np.float64(42.76), np.float64(45.68), np.float64(47.92), np.float64(51.54), np.float64(54.32), np.float64(56.84), np.float64(60.64), np.float64(63.34), np.float64(66.14), np.float64(69.72), np.float64(73.54), np.float64(75.36), np.float64(78.92), np.float64(81.5), np.float64(84.8), np.float64(88.1), np.float64(91.44), np.float64(94.08), np.float64(97.84), np.float64(101.62), np.float64(105.44), np.float64(109.16), np.float64(112.98), np.float64(116.78), np.float64(120.38), np.float64(124.14), np.float64(127.98), np.float64(131.82), np.float64(135.52), np.float64(136.5), np.float64(139.52), np.float64(142.7), np.float64(145.18), np.float64(147.88), np.float64(150.38), np.float64(152.94), np.float64(155.1), np.float64(156.76), np.float64(158.96), np.float64(160.6), np.float64(163.24), np.float64(166.26), np.float64(166.74), np.float64(169.34), np.float64(172.3), np.float64(174.44), np.float64(177.22), np.float64(179.16), np.float64(181.54), np.float64(184.34), np.float64(186.22), np.float64(187.1), np.float64(188.68), np.float64(188.8), np.float64(189.56), np.float64(190.86), np.float64(190.86), np.float64(191.82), np.float64(192.8), np.float64(193.02), np.float64(192.94), np.float64(194.5), np.float64(194.72), np.float64(195.92), np.float64(196.3), np.float64(196.76), np.float64(196.68), np.float64(197.04), np.float64(196.32), np.float64(196.32), np.float64(196.32), np.float64(196.32), np.float64(196.32), np.float64(195.58), np.float64(195.58), np.float64(195.58), np.float64(195.58), np.float64(195.42), np.float64(195.42), np.float64(195.42), np.float64(194.68), np.float64(194.68), np.float64(194.68), np.float64(194.1), np.float64(194.1), np.float64(193.24), np.float64(192.5), np.float64(192.5), np.float64(192.5), np.float64(193.28), np.float64(193.28), np.float64(192.68), np.float64(192.0), np.float64(191.86), np.float64(191.88), np.float64(191.88), np.float64(191.92), np.float64(191.72), np.float64(191.72), np.float64(191.4), np.float64(191.4), np.float64(191.4), np.float64(191.4), np.float64(191.4), np.float64(191.4), np.float64(191.4), np.float64(191.26), np.float64(191.38), np.float64(190.78), np.float64(190.74), np.float64(190.82), np.float64(191.22), np.float64(191.22), np.float64(191.22), np.float64(191.14), np.float64(190.3), np.float64(190.84), np.float64(191.42), np.float64(191.76), np.float64(191.64), np.float64(191.64), np.float64(191.64), np.float64(191.04), np.float64(191.78), np.float64(191.78), np.float64(191.3), np.float64(191.3), np.float64(191.46), np.float64(190.18), np.float64(190.18), np.float64(190.8), np.float64(190.8), np.float64(190.8), np.float64(191.38), np.float64(190.56), np.float64(190.82), np.float64(191.32)]\n",
        "Q_softmax_avg_reward = [np.float64(10.0), np.float64(9.5), np.float64(9.666666666666666), np.float64(9.5), np.float64(9.6), np.float64(9.5), np.float64(9.571428571428571), np.float64(9.625), np.float64(9.555555555555555), np.float64(9.5), np.float64(9.545454545454545), np.float64(9.583333333333334), np.float64(9.615384615384615), np.float64(9.571428571428571), np.float64(9.466666666666667), np.float64(9.5), np.float64(9.529411764705882), np.float64(9.555555555555555), np.float64(9.526315789473685), np.float64(9.55), np.float64(9.571428571428571), np.float64(9.590909090909092), np.float64(9.608695652173912), np.float64(9.583333333333334), np.float64(9.6), np.float64(9.576923076923077), np.float64(9.62962962962963), np.float64(9.642857142857142), np.float64(9.655172413793103), np.float64(9.6), np.float64(9.580645161290322), np.float64(9.5625), np.float64(9.545454545454545), np.float64(9.558823529411764), np.float64(9.571428571428571), np.float64(9.583333333333334), np.float64(9.594594594594595), np.float64(9.605263157894736), np.float64(9.58974358974359), np.float64(9.575), np.float64(9.585365853658537), np.float64(9.547619047619047), np.float64(9.55813953488372), np.float64(9.522727272727273), np.float64(9.555555555555555), np.float64(9.58695652173913), np.float64(9.595744680851064), np.float64(9.583333333333334), np.float64(9.571428571428571), np.float64(9.58), np.float64(9.56), np.float64(9.58), np.float64(9.56), np.float64(9.58), np.float64(9.56), np.float64(9.56), np.float64(9.56), np.float64(9.54), np.float64(9.54), np.float64(9.56), np.float64(9.56), np.float64(9.56), np.float64(9.58), np.float64(9.62), np.float64(9.62), np.float64(9.6), np.float64(9.56), np.float64(9.58), np.float64(9.58), np.float64(9.6), np.float64(9.58), np.float64(9.58), np.float64(9.56), np.float64(9.58), np.float64(9.56), np.float64(9.6), np.float64(9.6), np.float64(9.6), np.float64(9.58), np.float64(9.58), np.float64(9.58), np.float64(9.6), np.float64(9.9), np.float64(10.12), np.float64(10.34), np.float64(10.36), np.float64(10.62), np.float64(10.62), np.float64(10.66), np.float64(10.92), np.float64(11.82), np.float64(11.94), np.float64(12.06), np.float64(12.32), np.float64(12.36), np.float64(12.42), np.float64(12.48), np.float64(12.54), np.float64(12.54), np.float64(12.6), np.float64(12.7), np.float64(12.76), np.float64(12.82), np.float64(13.06), np.float64(13.22), np.float64(13.3), np.float64(13.46), np.float64(13.64), np.float64(13.68), np.float64(13.8), np.float64(14.02), np.float64(14.2), np.float64(14.48), np.float64(14.68), np.float64(14.92), np.float64(15.08), np.float64(15.28), np.float64(15.5), np.float64(15.96), np.float64(16.34), np.float64(16.86), np.float64(17.1), np.float64(17.36), np.float64(17.68), np.float64(18.34), np.float64(18.64), np.float64(18.86), np.float64(19.3), np.float64(19.54), np.float64(19.94), np.float64(20.42), np.float64(20.68), np.float64(20.84), np.float64(21.0), np.float64(21.42), np.float64(22.22), np.float64(22.66), np.float64(23.76), np.float64(24.6), np.float64(25.12), np.float64(25.24), np.float64(26.3), np.float64(28.04), np.float64(29.06), np.float64(30.3), np.float64(31.58), np.float64(33.04), np.float64(34.8), np.float64(36.36), np.float64(37.8), np.float64(39.28), np.float64(40.64), np.float64(42.26), np.float64(43.44), np.float64(44.8), np.float64(46.08), np.float64(47.32), np.float64(48.54), np.float64(50.16), np.float64(51.6), np.float64(53.12), np.float64(54.16), np.float64(55.28), np.float64(56.6), np.float64(58.04), np.float64(59.38), np.float64(60.58), np.float64(61.66), np.float64(62.8), np.float64(63.68), np.float64(64.68), np.float64(66.14), np.float64(67.48), np.float64(68.62), np.float64(69.54), np.float64(70.68), np.float64(71.7), np.float64(73.0), np.float64(74.4), np.float64(75.5), np.float64(76.36), np.float64(77.68), np.float64(78.74), np.float64(79.88), np.float64(80.82), np.float64(81.66), np.float64(82.44), np.float64(82.88), np.float64(83.86), np.float64(84.44), np.float64(84.84), np.float64(85.32), np.float64(85.1), np.float64(85.4), np.float64(85.7), np.float64(85.82), np.float64(85.98), np.float64(85.64), np.float64(85.84), np.float64(85.84)]"
      ],
      "metadata": {
        "id": "eC9AgOk2dpIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create x-axis values (episode numbers)\n",
        "episodes = range(len(Q_epsilon_greedy_avg_reward))  # Assuming all lists have the same length\n",
        "\n",
        "# Plot the data\n",
        "plt.plot(episodes, Q_epsilon_greedy_avg_reward, label=\"Q-learning (epsilon-greedy)\")\n",
        "plt.plot(episodes, Q_softmax_avg_reward, label=\"Q-learning (softmax)\")\n",
        "plt.plot(episodes, SARSA_epsilon_greedy_avg_reward, label=\"SARSA (epsilon-greedy)\")\n",
        "plt.plot(episodes, SARSA_softmax_avg_reward, label=\"SARSA (softmax)\")\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Average Reward (Last 50 Episodes)\")\n",
        "plt.title(\"Average Reward vs. Episode for Different Agents\")\n",
        "plt.grid(True)\n",
        "\n",
        "# Add legend\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FkaevBKfWQbG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}